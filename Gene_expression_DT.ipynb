{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Gene_expression.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiAyYSFE0YHL"
      },
      "source": [
        "# Histone modifications"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Krri7AXy8UM"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "random_seed = 123\n",
        "np.random.seed(random_seed)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7DJU4Tny8Ua"
      },
      "source": [
        "# 1. Reading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alZpE70qy8Uh"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv(\"https://raw.githubusercontent.com/sdgroeve/ML-course-VIB-2020/master/data_train.csv\")\n",
        "test = pd.read_csv(\"https://raw.githubusercontent.com/sdgroeve/ML-course-VIB-2020/master/data_test.csv\")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZuSUhLu4L8x"
      },
      "source": [
        "train_ids = train.pop(\"GeneId\")\n",
        "train_labels = train.pop(\"Label\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4VjbA3ey8VN"
      },
      "source": [
        "test_index_col = test.pop(\"GeneId\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FesoIiyNy8Vd"
      },
      "source": [
        "# 2. Fitting a decision tree model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaqTeiPjy8Vg"
      },
      "source": [
        "The scikit-learn `DecisionTreeClassifier` class computes a decision tree predictive model from a dataset. \n",
        "\n",
        "To get all the options for learning you can simply type: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGi4kfLzy8Vh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48b7d8d8-0cd5-460d-be13-26d72c4508d3"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "help(DecisionTreeClassifier)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class DecisionTreeClassifier in module sklearn.tree._classes:\n",
            "\n",
            "class DecisionTreeClassifier(sklearn.base.ClassifierMixin, BaseDecisionTree)\n",
            " |  A decision tree classifier.\n",
            " |  \n",
            " |  Read more in the :ref:`User Guide <tree>`.\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
            " |      The function to measure the quality of a split. Supported criteria are\n",
            " |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
            " |  \n",
            " |  splitter : {\"best\", \"random\"}, default=\"best\"\n",
            " |      The strategy used to choose the split at each node. Supported\n",
            " |      strategies are \"best\" to choose the best split and \"random\" to choose\n",
            " |      the best random split.\n",
            " |  \n",
            " |  max_depth : int, default=None\n",
            " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
            " |      all leaves are pure or until all leaves contain less than\n",
            " |      min_samples_split samples.\n",
            " |  \n",
            " |  min_samples_split : int or float, default=2\n",
            " |      The minimum number of samples required to split an internal node:\n",
            " |  \n",
            " |      - If int, then consider `min_samples_split` as the minimum number.\n",
            " |      - If float, then `min_samples_split` is a fraction and\n",
            " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
            " |        number of samples for each split.\n",
            " |  \n",
            " |      .. versionchanged:: 0.18\n",
            " |         Added float values for fractions.\n",
            " |  \n",
            " |  min_samples_leaf : int or float, default=1\n",
            " |      The minimum number of samples required to be at a leaf node.\n",
            " |      A split point at any depth will only be considered if it leaves at\n",
            " |      least ``min_samples_leaf`` training samples in each of the left and\n",
            " |      right branches.  This may have the effect of smoothing the model,\n",
            " |      especially in regression.\n",
            " |  \n",
            " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
            " |      - If float, then `min_samples_leaf` is a fraction and\n",
            " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
            " |        number of samples for each node.\n",
            " |  \n",
            " |      .. versionchanged:: 0.18\n",
            " |         Added float values for fractions.\n",
            " |  \n",
            " |  min_weight_fraction_leaf : float, default=0.0\n",
            " |      The minimum weighted fraction of the sum total of weights (of all\n",
            " |      the input samples) required to be at a leaf node. Samples have\n",
            " |      equal weight when sample_weight is not provided.\n",
            " |  \n",
            " |  max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\n",
            " |      The number of features to consider when looking for the best split:\n",
            " |  \n",
            " |          - If int, then consider `max_features` features at each split.\n",
            " |          - If float, then `max_features` is a fraction and\n",
            " |            `int(max_features * n_features)` features are considered at each\n",
            " |            split.\n",
            " |          - If \"auto\", then `max_features=sqrt(n_features)`.\n",
            " |          - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
            " |          - If \"log2\", then `max_features=log2(n_features)`.\n",
            " |          - If None, then `max_features=n_features`.\n",
            " |  \n",
            " |      Note: the search for a split does not stop until at least one\n",
            " |      valid partition of the node samples is found, even if it requires to\n",
            " |      effectively inspect more than ``max_features`` features.\n",
            " |  \n",
            " |  random_state : int or RandomState, default=None\n",
            " |      If int, random_state is the seed used by the random number generator;\n",
            " |      If RandomState instance, random_state is the random number generator;\n",
            " |      If None, the random number generator is the RandomState instance used\n",
            " |      by `np.random`.\n",
            " |  \n",
            " |  max_leaf_nodes : int, default=None\n",
            " |      Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
            " |      Best nodes are defined as relative reduction in impurity.\n",
            " |      If None then unlimited number of leaf nodes.\n",
            " |  \n",
            " |  min_impurity_decrease : float, default=0.0\n",
            " |      A node will be split if this split induces a decrease of the impurity\n",
            " |      greater than or equal to this value.\n",
            " |  \n",
            " |      The weighted impurity decrease equation is the following::\n",
            " |  \n",
            " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
            " |                              - N_t_L / N_t * left_impurity)\n",
            " |  \n",
            " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
            " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
            " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
            " |  \n",
            " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
            " |      if ``sample_weight`` is passed.\n",
            " |  \n",
            " |      .. versionadded:: 0.19\n",
            " |  \n",
            " |  min_impurity_split : float, default=1e-7\n",
            " |      Threshold for early stopping in tree growth. A node will split\n",
            " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
            " |  \n",
            " |      .. deprecated:: 0.19\n",
            " |         ``min_impurity_split`` has been deprecated in favor of\n",
            " |         ``min_impurity_decrease`` in 0.19. The default value of\n",
            " |         ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n",
            " |         will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
            " |  \n",
            " |  class_weight : dict, list of dict or \"balanced\", default=None\n",
            " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
            " |      If None, all classes are supposed to have weight one. For\n",
            " |      multi-output problems, a list of dicts can be provided in the same\n",
            " |      order as the columns of y.\n",
            " |  \n",
            " |      Note that for multioutput (including multilabel) weights should be\n",
            " |      defined for each class of every column in its own dict. For example,\n",
            " |      for four-class multilabel classification weights should be\n",
            " |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
            " |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
            " |  \n",
            " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
            " |      weights inversely proportional to class frequencies in the input data\n",
            " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
            " |  \n",
            " |      For multi-output, the weights of each column of y will be multiplied.\n",
            " |  \n",
            " |      Note that these weights will be multiplied with sample_weight (passed\n",
            " |      through the fit method) if sample_weight is specified.\n",
            " |  \n",
            " |  presort : deprecated, default='deprecated'\n",
            " |      This parameter is deprecated and will be removed in v0.24.\n",
            " |  \n",
            " |      .. deprecated:: 0.22\n",
            " |  \n",
            " |  ccp_alpha : non-negative float, default=0.0\n",
            " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
            " |      subtree with the largest cost complexity that is smaller than\n",
            " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
            " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
            " |  \n",
            " |      .. versionadded:: 0.22\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  classes_ : ndarray of shape (n_classes,) or list of ndarray\n",
            " |      The classes labels (single output problem),\n",
            " |      or a list of arrays of class labels (multi-output problem).\n",
            " |  \n",
            " |  feature_importances_ : ndarray of shape (n_features,)\n",
            " |      The feature importances. The higher, the more important the\n",
            " |      feature. The importance of a feature is computed as the (normalized)\n",
            " |      total reduction of the criterion brought by that feature.  It is also\n",
            " |      known as the Gini importance [4]_.\n",
            " |  \n",
            " |  max_features_ : int\n",
            " |      The inferred value of max_features.\n",
            " |  \n",
            " |  n_classes_ : int or list of int\n",
            " |      The number of classes (for single output problems),\n",
            " |      or a list containing the number of classes for each\n",
            " |      output (for multi-output problems).\n",
            " |  \n",
            " |  n_features_ : int\n",
            " |      The number of features when ``fit`` is performed.\n",
            " |  \n",
            " |  n_outputs_ : int\n",
            " |      The number of outputs when ``fit`` is performed.\n",
            " |  \n",
            " |  tree_ : Tree\n",
            " |      The underlying Tree object. Please refer to\n",
            " |      ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n",
            " |      :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n",
            " |      for basic usage of these attributes.\n",
            " |  \n",
            " |  See Also\n",
            " |  --------\n",
            " |  DecisionTreeRegressor : A decision tree regressor.\n",
            " |  \n",
            " |  Notes\n",
            " |  -----\n",
            " |  The default values for the parameters controlling the size of the trees\n",
            " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
            " |  unpruned trees which can potentially be very large on some data sets. To\n",
            " |  reduce memory consumption, the complexity and size of the trees should be\n",
            " |  controlled by setting those parameter values.\n",
            " |  \n",
            " |  The features are always randomly permuted at each split. Therefore,\n",
            " |  the best found split may vary, even with the same training data and\n",
            " |  ``max_features=n_features``, if the improvement of the criterion is\n",
            " |  identical for several splits enumerated during the search of the best\n",
            " |  split. To obtain a deterministic behaviour during fitting,\n",
            " |  ``random_state`` has to be fixed.\n",
            " |  \n",
            " |  References\n",
            " |  ----------\n",
            " |  \n",
            " |  .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
            " |  \n",
            " |  .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
            " |         and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
            " |  \n",
            " |  .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
            " |         Learning\", Springer, 2009.\n",
            " |  \n",
            " |  .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
            " |         https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> from sklearn.datasets import load_iris\n",
            " |  >>> from sklearn.model_selection import cross_val_score\n",
            " |  >>> from sklearn.tree import DecisionTreeClassifier\n",
            " |  >>> clf = DecisionTreeClassifier(random_state=0)\n",
            " |  >>> iris = load_iris()\n",
            " |  >>> cross_val_score(clf, iris.data, iris.target, cv=10)\n",
            " |  ...                             # doctest: +SKIP\n",
            " |  ...\n",
            " |  array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n",
            " |          0.93...,  0.93...,  1.     ,  0.93...,  1.      ])\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      DecisionTreeClassifier\n",
            " |      sklearn.base.ClassifierMixin\n",
            " |      BaseDecisionTree\n",
            " |      sklearn.base.MultiOutputMixin\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort='deprecated', ccp_alpha=0.0)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  fit(self, X, y, sample_weight=None, check_input=True, X_idx_sorted=None)\n",
            " |      Build a decision tree classifier from the training set (X, y).\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The training input samples. Internally, it will be converted to\n",
            " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
            " |          to a sparse ``csc_matrix``.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          The target values (class labels) as integers or strings.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights. If None, then samples are equally weighted. Splits\n",
            " |          that would create child nodes with net zero or negative weight are\n",
            " |          ignored while searching for a split in each node. Splits are also\n",
            " |          ignored if they would result in any single class carrying a\n",
            " |          negative weight in either child node.\n",
            " |      \n",
            " |      check_input : bool, default=True\n",
            " |          Allow to bypass several input checking.\n",
            " |          Don't use this parameter unless you know what you do.\n",
            " |      \n",
            " |      X_idx_sorted : array-like of shape (n_samples, n_features),                 default=None\n",
            " |          The indexes of the sorted training input samples. If many tree\n",
            " |          are grown on the same dataset, this allows the ordering to be\n",
            " |          cached between trees. If None, the data will be sorted here.\n",
            " |          Don't use this parameter unless you know what to do.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : DecisionTreeClassifier\n",
            " |          Fitted estimator.\n",
            " |  \n",
            " |  predict_log_proba(self, X)\n",
            " |      Predict class log-probabilities of the input samples X.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, it will be converted to\n",
            " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
            " |          to a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      proba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs > 1\n",
            " |          The class log-probabilities of the input samples. The order of the\n",
            " |          classes corresponds to that in the attribute :term:`classes_`.\n",
            " |  \n",
            " |  predict_proba(self, X, check_input=True)\n",
            " |      Predict class probabilities of the input samples X.\n",
            " |      \n",
            " |      The predicted class probability is the fraction of samples of the same\n",
            " |      class in a leaf.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, it will be converted to\n",
            " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
            " |          to a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      check_input : bool, default=True\n",
            " |          Allow to bypass several input checking.\n",
            " |          Don't use this parameter unless you know what you do.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      proba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs > 1\n",
            " |          The class probabilities of the input samples. The order of the\n",
            " |          classes corresponds to that in the attribute :term:`classes_`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  score(self, X, y, sample_weight=None)\n",
            " |      Return the mean accuracy on the given test data and labels.\n",
            " |      \n",
            " |      In multi-label classification, this is the subset accuracy\n",
            " |      which is a harsh metric since you require for each sample that\n",
            " |      each label set be correctly predicted.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Test samples.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          True labels for X.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      score : float\n",
            " |          Mean accuracy of self.predict(X) wrt. y.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from BaseDecisionTree:\n",
            " |  \n",
            " |  apply(self, X, check_input=True)\n",
            " |      Return the index of the leaf that each sample is predicted as.\n",
            " |      \n",
            " |      .. versionadded:: 0.17\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, it will be converted to\n",
            " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
            " |          to a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      check_input : bool, default=True\n",
            " |          Allow to bypass several input checking.\n",
            " |          Don't use this parameter unless you know what you do.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      X_leaves : array-like of shape (n_samples,)\n",
            " |          For each datapoint x in X, return the index of the leaf x\n",
            " |          ends up in. Leaves are numbered within\n",
            " |          ``[0; self.tree_.node_count)``, possibly with gaps in the\n",
            " |          numbering.\n",
            " |  \n",
            " |  cost_complexity_pruning_path(self, X, y, sample_weight=None)\n",
            " |      Compute the pruning path during Minimal Cost-Complexity Pruning.\n",
            " |      \n",
            " |      See :ref:`minimal_cost_complexity_pruning` for details on the pruning\n",
            " |      process.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The training input samples. Internally, it will be converted to\n",
            " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
            " |          to a sparse ``csc_matrix``.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          The target values (class labels) as integers or strings.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights. If None, then samples are equally weighted. Splits\n",
            " |          that would create child nodes with net zero or negative weight are\n",
            " |          ignored while searching for a split in each node. Splits are also\n",
            " |          ignored if they would result in any single class carrying a\n",
            " |          negative weight in either child node.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      ccp_path : Bunch\n",
            " |          Dictionary-like object, with attributes:\n",
            " |      \n",
            " |          ccp_alphas : ndarray\n",
            " |              Effective alphas of subtree during pruning.\n",
            " |      \n",
            " |          impurities : ndarray\n",
            " |              Sum of the impurities of the subtree leaves for the\n",
            " |              corresponding alpha value in ``ccp_alphas``.\n",
            " |  \n",
            " |  decision_path(self, X, check_input=True)\n",
            " |      Return the decision path in the tree.\n",
            " |      \n",
            " |      .. versionadded:: 0.18\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, it will be converted to\n",
            " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
            " |          to a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      check_input : bool, default=True\n",
            " |          Allow to bypass several input checking.\n",
            " |          Don't use this parameter unless you know what you do.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
            " |          Return a node indicator CSR matrix where non zero elements\n",
            " |          indicates that the samples goes through the nodes.\n",
            " |  \n",
            " |  get_depth(self)\n",
            " |      Return the depth of the decision tree.\n",
            " |      \n",
            " |      The depth of a tree is the maximum distance between the root\n",
            " |      and any leaf.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self.tree_.max_depth : int\n",
            " |          The maximum depth of the tree.\n",
            " |  \n",
            " |  get_n_leaves(self)\n",
            " |      Return the number of leaves of the decision tree.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self.tree_.n_leaves : int\n",
            " |          Number of leaves.\n",
            " |  \n",
            " |  predict(self, X, check_input=True)\n",
            " |      Predict class or regression value for X.\n",
            " |      \n",
            " |      For a classification model, the predicted class for each sample in X is\n",
            " |      returned. For a regression model, the predicted value based on X is\n",
            " |      returned.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, it will be converted to\n",
            " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
            " |          to a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      check_input : bool, default=True\n",
            " |          Allow to bypass several input checking.\n",
            " |          Don't use this parameter unless you know what you do.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          The predicted classes, or the predict values.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from BaseDecisionTree:\n",
            " |  \n",
            " |  feature_importances_\n",
            " |      Return the feature importances.\n",
            " |      \n",
            " |      The importance of a feature is computed as the (normalized) total\n",
            " |      reduction of the criterion brought by that feature.\n",
            " |      It is also known as the Gini importance.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      feature_importances_ : ndarray of shape (n_features,)\n",
            " |          Normalized total reduction of criteria by feature\n",
            " |          (Gini importance).\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool, default=True\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : mapping of string to any\n",
            " |          Parameter names mapped to their values.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |      \n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as pipelines). The latter have parameters of the form\n",
            " |      ``<component>__<parameter>`` so that it's possible to update each\n",
            " |      component of a nested object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : dict\n",
            " |          Estimator parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          Estimator instance.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNn63QiNy8Vo"
      },
      "source": [
        "You notice that there are many (hyper)parameters to set. These influence the complexity of the model. An important such parameter is the `max_depth` that sets a limit on how deep a decision tree can become. \n",
        "\n",
        "Let's create a decision tree model with `max_depth=3`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0Pjoa08y8Vp"
      },
      "source": [
        "cls = DecisionTreeClassifier(max_depth=3)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-0bRK0ky8Vy"
      },
      "source": [
        "This creates a decision tree model with default values for the other hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vVO8Qnuy8V0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee24d28f-8f45-4f16-9f51-f2397d7575b9"
      },
      "source": [
        "cls"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                       max_depth=3, max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYVjJV6Ay8WF"
      },
      "source": [
        "Let's create a validation set, fit the model and evaluate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPl3orbDy8WH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d42b64ed-da30-4a7e-86d3-1034ba221817"
      },
      "source": [
        "from sklearn.metrics import log_loss, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_X, val_X, train_y, val_y = train_test_split(train,train_labels,\n",
        "                                                  test_size=.2, random_state=random_seed)\n",
        "\n",
        "cls.fit(train_X,train_y)\n",
        "\n",
        "predictions_train = cls.predict(train_X)\n",
        "predictions_val = cls.predict(val_X)\n",
        "\n",
        "print(\"Accuracy: (%f) %f\"%(accuracy_score(predictions_train, train_y),accuracy_score(predictions_val, val_y)))\n",
        "\n",
        "predictions_train_prob = cls.predict_proba(train_X)\n",
        "predictions_val_prob = cls.predict_proba(val_X)\n",
        "\n",
        "print(\"Log-loss: (%f) %f\"%(log_loss(train_y,predictions_train_prob[:,-1]),log_loss(val_y,predictions_val_prob[:,-1])))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: (0.822353) 0.818008\n",
            "Log-loss: (0.456446) 0.461147\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZcpHy1My8Wj"
      },
      "source": [
        "The following code plots the fitted decision tree `cls` as a `tree.png` file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_H7Nlqs9y8Wl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "dec67bba-82aa-4d59-bb5c-9c62571ace6f"
      },
      "source": [
        "\"\"\"\n",
        "from sklearn import tree\n",
        "from io import StringIO\n",
        "from IPython.display import Image, display\n",
        "import pydotplus\n",
        "\n",
        "out = StringIO()\n",
        "tree.export_graphviz(cls_DT, out_file=out)\n",
        "graph=pydotplus.graph_from_dot_data(out.getvalue())\n",
        "graph.write_png(\"tree.png\")\n",
        "\"\"\""
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfrom sklearn import tree\\nfrom io import StringIO\\nfrom IPython.display import Image, display\\nimport pydotplus\\n\\nout = StringIO()\\ntree.export_graphviz(cls_DT, out_file=out)\\ngraph=pydotplus.graph_from_dot_data(out.getvalue())\\ngraph.write_png(\"tree.png\")\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLYKBq2u4VJT",
        "outputId": "c5ba61b7-f821-42b4-94b1-56e5bee91f91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "result = []\n",
        "for md in range(1,10):\n",
        "  cls = DecisionTreeClassifier(max_depth=md)\n",
        "  cls.fit(train_X,train_y)\n",
        "  predictions_train_prob = cls.predict_proba(train_X)\n",
        "  predictions_val_prob = cls.predict_proba(val_X)\n",
        "  result.append([log_loss(train_y,predictions_train_prob[:,-1]),log_loss(val_y,predictions_val_prob[:,-1])])\n",
        "\n",
        "toplot = pd.DataFrame(result,columns=[\"train\",\"validation\"])\n",
        "toplot.plot()\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7efebae7cda0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iSXs1tay8Wq"
      },
      "source": [
        "# 4. Kaggle evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mclUVPCAy8Ws"
      },
      "source": [
        "Let's make a Kaggle submission:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS-kgBKAy8Wt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "outputId": "18b28f9b-3d39-48fe-e073-61774f2b0ca2"
      },
      "source": [
        "#code for submission\n",
        "predictions_test = cls_DT.predict_proba(test)\n",
        "\n",
        "out_tmp = pd.DataFrame()\n",
        "out_tmp[\"GeneId\"] = test_index_col\n",
        "out_tmp[\"eyeDetection\"] = predictions_test[:,1]\n",
        "out_tmp.to_csv(\"submission_DT.csv\",index=False)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-23482c346bbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#code for submission\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictions_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls_DT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mout_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mout_tmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"GeneId\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_index_col\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cls_DT' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl8OTjbZKvMp"
      },
      "source": [
        "How does the model perform on the testset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9DhKikmy8W2"
      },
      "source": [
        "# 5. Hyperparamters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ_O4Ezmy8W3"
      },
      "source": [
        "For our first submission we set the hyperparameter `max_depth=3`. Other values might result in lower log-loss on the testset. \n",
        "\n",
        "Since we don't have the testset labels we can only check this on the public leaderboard, which we can/should not do!\n",
        "\n",
        "So, we need to create our own testset (**not seen during training!**) with known class labels.\n",
        "\n",
        "Scikit-learn offers many options to do this. One of them is the `train_test_split` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-U7t7hZy8W7"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_X, val_X, train_y, val_y = train_test_split(train_norm,train_labels,\n",
        "                                                  test_size=.2, random_state=random_seed)\n",
        "\n",
        "#train fold\n",
        "print(train_X.shape)\n",
        "print(train_y.shape)\n",
        "#validation fold\n",
        "print(val_X.shape)\n",
        "print(val_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEv2x3Smy8XB"
      },
      "source": [
        "Fit a decision tree model with `max_depth=14` default paramters on the `train_X` data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx29OyFly8XD"
      },
      "source": [
        "#solution\n",
        "cls_DT = DecisionTreeClassifier(max_depth=14)\n",
        "cls_DT.fit(train_X,train_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH9q5De8y8XI"
      },
      "source": [
        "What is the accuracy and log-loss on `train_X`? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW3HjC5Dy8XJ"
      },
      "source": [
        "#solution\n",
        "predictions = cls_DT.predict(train_X)\n",
        "print(\"Accuracy: %f\"%(accuracy_score(predictions, train_y)))\n",
        "predictions = cls_DT.predict_proba(train_X)\n",
        "print(\"Log-loss: %f\"%(log_loss(train_y,predictions[:,1])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqHXtasSy8XO"
      },
      "source": [
        "What is the accuracy and log-loss on `val_X`?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm7GVlo-y8XQ"
      },
      "source": [
        "#solution\n",
        "predictions = cls_DT.predict(val_X)\n",
        "print(\"Accuracy: %f\"%(accuracy_score(predictions, val_y)))\n",
        "predictions = cls_DT.predict_proba(val_X)\n",
        "print(\"Log-loss: %f\"%(log_loss(val_y,predictions[:,1])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1p535f2y8XY"
      },
      "source": [
        "What do you see?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBRBR7Q4y8Xd"
      },
      "source": [
        "The following code evaluates different values for this hyperparameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6avhhiiy8Xe"
      },
      "source": [
        "for maxdepth in range(1,20,1):\n",
        "    cls = DecisionTreeClassifier(max_depth=maxdepth)\n",
        "    cls.fit(train_X,train_y)\n",
        "    predictions_train = cls.predict(train_X)\n",
        "    predictions_val = cls.predict(val_X)\n",
        "    predictions_train_prob = cls.predict_proba(train_X)[:,1]\n",
        "    predictions_val_prob = cls.predict_proba(val_X)[:,1]\n",
        "    print(\"%i (%f) %f (%f) %f\"%(maxdepth,\n",
        "                                accuracy_score(predictions_train, train_y),accuracy_score(predictions_val, val_y),\n",
        "                                log_loss(train_y,predictions_train_prob),log_loss(val_y,predictions_val_prob)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7lmtHBQvuon"
      },
      "source": [
        "What do you see?\n",
        "\n",
        "So, we have split the data into a train- and validationset. We can of course split the data in many different ways (different random seeds) resulting in different train- and validationsets.\n",
        "\n",
        "Let's try 5 different random seeds:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOxT7dLan5Nr"
      },
      "source": [
        "for run in range(5):\n",
        "  train_X, val_X, train_y, val_y = train_test_split(train_norm,train_labels,\n",
        "                                                  test_size=.8, random_state=run)\n",
        "  min_m = 100\n",
        "  best = None\n",
        "  for maxdepth in range(1,20,1):\n",
        "      cls = DecisionTreeClassifier(max_depth=maxdepth)\n",
        "      cls.fit(train_X,train_y)\n",
        "      predictions_val_prob = cls.predict_proba(val_X)[:,1]\n",
        "      m = log_loss(val_y,predictions_val_prob)\n",
        "      if m < min_m:\n",
        "        min_m = m\n",
        "        best = maxdepth\n",
        "  print(\"%i %f\"%(best,min_m))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQgyvTfTwN8G"
      },
      "source": [
        "What do you see?\n",
        "\n",
        "The solution is to run several train-validations splits and average the performance.\n",
        "\n",
        "One popular method is cross-validation that uses each datapoint once as a testpoint.\n",
        "\n",
        "It works as follows:\n",
        "<br/>\n",
        "<br/>\n",
        "<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\"/>\n",
        "<br/>\n",
        "<br/>\n",
        "\n",
        "It is easy to run this in Scikit-learn:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhFFLaJ_sG9L"
      },
      "source": [
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "for maxdepth in range(1,10,1):\n",
        "    cls = DecisionTreeClassifier(max_depth=maxdepth)\n",
        "    predictions = cross_val_predict(cls,train_norm,train_labels,\n",
        "                                    cv=10,\n",
        "                                    method=\"predict_proba\")\n",
        "    print(\"%i %f\"%(maxdepth,log_loss(train_labels,predictions[:,1])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87qY5lFuyEms"
      },
      "source": [
        "We can do this in two lines of code with the `GridSearchCV` module:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgVOx2SLsIGV"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {\n",
        "    'max_depth':range(1,10)\n",
        "    }\n",
        "\n",
        "GSCV = GridSearchCV(cls_DT, params,\n",
        "                    cv=10,\n",
        "                    scoring=\"neg_log_loss\",\n",
        "                    verbose=1).fit(train_norm,train_labels)\n",
        "\n",
        "print(GSCV.best_estimator_)\n",
        "print(GSCV.best_score_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUgY_6Ply8X3"
      },
      "source": [
        "Play with the hyperparameters in a Template notebook and make some Kaggle submissions.\n",
        "\n",
        "# 5. Ensemble learning: bagging\n",
        "\n",
        "We have seen that bias and variance play an important role in Machine Learning. \n",
        "\n",
        "Let's first see what bagging can do for our dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m16JfepBy8X4"
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "cls = BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=random_seed)\n",
        "                                                            \n",
        "cls.fit(train_X,train_y)\n",
        "predictions_train = cls.predict(train_X)\n",
        "predictions_val = cls.predict(val_X)\n",
        "print(\"Accuracy: (%f) %f\"%(accuracy_score(predictions_train, train_y),accuracy_score(predictions_val, val_y)))\n",
        "predictions_train_prob = cls.predict_proba(train_X)\n",
        "predictions_val_prob = cls.predict_proba(val_X)\n",
        "print(\"Log-loss: (%f) %f\"%(log_loss(train_y,predictions_train_prob[:,1]),log_loss(val_y,predictions_val_prob[:,1])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsA-43z8y8X9"
      },
      "source": [
        "With the `RandomForestClassifier` the variance of the decision tree is reduced also by selecting features for decision tree contruction at random. Let's see how far we get with default hyperparameter values.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIfSnbtGy8X-"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "cls = RandomForestClassifier(random_state=random_seed)\n",
        "\n",
        "cls.fit(train_X,train_y)\n",
        "predictions_train = cls.predict(train_X)\n",
        "predictions_val = cls.predict(val_X)\n",
        "print(\"Accuracy: (%f) %f\"%(accuracy_score(predictions_train, train_y),accuracy_score(predictions_val, val_y)))\n",
        "predictions_train_prob = cls.predict_proba(train_X)\n",
        "predictions_val_prob = cls.predict_proba(val_X)\n",
        "print(\"Log-loss: (%f) %f\"%(log_loss(train_y,predictions_train_prob[:,1]),log_loss(val_y,predictions_val_prob[:,1])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn-e3Gujy8Ya"
      },
      "source": [
        "# 6. Ensemble learning: boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI1CdRroy8Yc"
      },
      "source": [
        "How about the `GradientBoostingClassifier`?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hR1KZv_y8Yd"
      },
      "source": [
        "#solution\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "cls = GradientBoostingClassifier(random_state=random_seed,\n",
        "                                    max_depth=10)\n",
        "cls.fit(train_X,train_y)\n",
        "predictions_train = cls.predict(train_X)\n",
        "predictions_val = cls.predict(val_X)\n",
        "print(\"Accuracy: (%f) %f\"%(accuracy_score(predictions_train, train_y),accuracy_score(predictions_val, val_y)))\n",
        "predictions_train_prob = cls.predict_proba(train_X)\n",
        "predictions_val_prob = cls.predict_proba(val_X)\n",
        "print(\"Log-loss: (%f) %f\"%(log_loss(train_y,predictions_train_prob[:,1]),log_loss(val_y,predictions_val_prob[:,1])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIg3wxby4gOd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}