{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Gene_expression.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Krri7AXy8UM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "random_seed = 123\n",
        "np.random.seed(random_seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7DJU4Tny8Ua",
        "colab_type": "text"
      },
      "source": [
        "# 1. Reading the data\n",
        "\n",
        "Histone modifications play an important role in affecting gene regulation. Specific histone modifications at specific locations in or near the gene can alter the expression of genes. Predicting gene expression from histone modification signals is a widely studied research topic.\n",
        "\n",
        "In this competition you will predict gene expression levels (low=0, high=1) based on the presence of histone modifications at specific locations in the gene. You will try to find the model that learns the true underlying model best.\n",
        "\n",
        "For each gene a region of 10.000bp around the transcription start site of the gene is extracted (5000bp upstream and 5000bp downstream). This region is binned in 100 bins of 100bp. For each bin five core histone modification marks are counted [1].\n",
        "\n",
        "The dataset is compiled from the \"E047\" (Primary T CD8+ naive cells from peripheral blood) celltype from Roadmap Epigenomics Mapping Consortium (REMC) database.\n",
        "\n",
        "[1] Kundaje, A. et al. Integrative analysis of 111 reference human epige-\n",
        "nomes. Nature, 518, 317â€“330, 2015.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fEaghfDy8Ue",
        "colab_type": "text"
      },
      "source": [
        "We start by loading the Pandas library and reading the datasets into Pandas DataFrames:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alZpE70qy8Uh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv(\"https://raw.githubusercontent.com/sdgroeve/ML-course-VIB-2020/master/data_train.csv\")\n",
        "test = pd.read_csv(\"https://raw.githubusercontent.com/sdgroeve/ML-course-VIB-2020/master/data_test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WA6pU9qDy8U0",
        "colab_type": "text"
      },
      "source": [
        "Let's look at the first 5 rows of the trainset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBIQBcTTy8U1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYHeBhf8y8VC",
        "colab_type": "text"
      },
      "source": [
        "There is a column called `GeneId` that identifies the gene. This column should not be used as a feature. \n",
        "\n",
        "The label for each datapoint is in the `Label` column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZuSUhLu4L8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ids = train.pop(\"GeneId\")\n",
        "train_labels = train.pop(\"Label\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DHXHPpo4SBG",
        "colab_type": "text"
      },
      "source": [
        "Now `train` contains the feature columns only.\n",
        "\n",
        "Let's look at the number datapoints in each class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynUhI2de6TsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4eZlOyO6NNF",
        "colab_type": "text"
      },
      "source": [
        "Let's look at `test`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDV9hz5Ay8VD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mao3WMV3y8VJ",
        "colab_type": "text"
      },
      "source": [
        "This is a blind test so the `Label` column is not available in the testset. The testset does contain the `GeneId` column that is needed to send your predictions to the Kaggle website.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4VjbA3ey8VN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_index_col = test.pop(\"GeneId\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbiH1uzSy8VT",
        "colab_type": "text"
      },
      "source": [
        "We can get some general statistics about the trainset using the DataFrame `.describe()` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPg1lbiWy8VV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.describe().transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kDGcmkU2KE5",
        "colab_type": "text"
      },
      "source": [
        "# 2. Data pre-processing\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oltcoOqH3TTD",
        "colab_type": "text"
      },
      "source": [
        "We can use the Pandas `boxplot()` function to plot the feature values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjZd6rZs3ijb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(22,8))\n",
        "train.boxplot()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsQjqutNtbP7",
        "colab_type": "text"
      },
      "source": [
        "Let's plot these for each hisotone mark:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv7vShayzrR8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "marks = {}\n",
        "for m in train.columns:\n",
        "  marks[m.split(\"_\")[0]] = True\n",
        "marks = list(marks.keys())\n",
        "marks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIwcZXHX0lO-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for mark in marks:\n",
        "  cols = []\n",
        "  for m in train.columns:\n",
        "    if mark in m:\n",
        "      cols.append(m)\n",
        "  plt.figure(figsize=(22,8))    \n",
        "  train[cols].boxplot()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNCXrS1x4Tf8",
        "colab_type": "text"
      },
      "source": [
        "In this case I choose scaling the features to [0,1]:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGkTWm904gt9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "scaler_minmax = preprocessing.MinMaxScaler()\n",
        "scaler_minmax.fit(train)\n",
        "train_norm = pd.DataFrame(scaler_minmax.transform(train),columns=train.columns)\n",
        "train_norm.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYY3fLMq5RfO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for mark in marks:\n",
        "  cols = []\n",
        "  for m in train_norm.columns:\n",
        "    if mark in m:\n",
        "      cols.append(m)\n",
        "  plt.figure(figsize=(22,8))    \n",
        "  train_norm[cols].boxplot()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FesoIiyNy8Vd",
        "colab_type": "text"
      },
      "source": [
        "# 3. Building a decision tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaqTeiPjy8Vg",
        "colab_type": "text"
      },
      "source": [
        "The scikit-learn `DecisionTreeClassifier` class computes a decision tree predictive model from a dataset. \n",
        "\n",
        "To get all the options for learning you can simply type: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGi4kfLzy8Vh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "help(DecisionTreeClassifier)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNn63QiNy8Vo",
        "colab_type": "text"
      },
      "source": [
        "You notice that there are many (hyper)parameters to set. These influence the complexity of the model. An important such parameter is the `max_depth` that set a limit on how deep a decision tree can become. \n",
        "\n",
        "Let's create a decision tree model with `max_depth=3`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0Pjoa08y8Vp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cls_DT = DecisionTreeClassifier(max_depth=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-0bRK0ky8Vy",
        "colab_type": "text"
      },
      "source": [
        "This creates a decision tree model with default values for the other hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vVO8Qnuy8V0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cls_DT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYVjJV6Ay8WF",
        "colab_type": "text"
      },
      "source": [
        "Now we can fit the trainset in just one line of code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPl3orbDy8WH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cls_DT.fit(train_norm,train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBEx3Mv-y8WQ",
        "colab_type": "text"
      },
      "source": [
        "Let's see how well the decision tree performs on the trainset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZR5qelPYy8WR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "predictions = cls_DT.predict(train_norm)\n",
        "print(\"Accuracy: %f\"%(accuracy_score(predictions, train_labels)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_Dq8upQy8WX",
        "colab_type": "text"
      },
      "source": [
        "These are the predictions for the first 100 data points:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRqE5_yHy8WX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions[0:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTaVvaUWI4_R",
        "colab_type": "text"
      },
      "source": [
        "For the Kaggle competition your predictions are not evaluated by accuracy, but by log-loss:\n",
        "\n",
        "$$ - \\frac{1}{N} \\sum_{i=1}^N [y_{i} \\log \\, p_{i} + (1 - y_{i}) \\log \\, (1 - p_{i})],$$\n",
        "\n",
        "where $N$ is the number of datapoints, $y_i$ is the label of datapoint $i$, and $p_i$ is the prediction of the model expressed as a probability.\n",
        "\n",
        "Let's compute the log-loss:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42cx7I0RetRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import log_loss\n",
        "\n",
        "print(\"Log-loss: %f\"%log_loss(train_labels,predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dW3UFe1te5sZ",
        "colab_type": "text"
      },
      "source": [
        "This is a very high log-loss (note that we want the log-loss the be as close to zero as possible).\n",
        "\n",
        "The reason is that we don't actually provide probabilities but just the classes 0 and 1. Any error will be punished with a very high contribution to the log-loss. Therefor, we should predict class probabilities rather than just classes. \n",
        "\n",
        "Decision trees allow us to do that:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8oluzNCJQq6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = cls_DT.predict_proba(train_norm)\n",
        "\n",
        "print(\"Log-loss: %f\"%log_loss(train_labels,predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LixpdZ7UfuH6",
        "colab_type": "text"
      },
      "source": [
        "Now the log-loss is much smaller."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZcpHy1My8Wj",
        "colab_type": "text"
      },
      "source": [
        "The following code plots the fitted decision tree `cls` as a `tree.png` file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_H7Nlqs9y8Wl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "from sklearn import tree\n",
        "from io import StringIO\n",
        "from IPython.display import Image, display\n",
        "import pydotplus\n",
        "\n",
        "out = StringIO()\n",
        "tree.export_graphviz(cls_DT, out_file=out)\n",
        "graph=pydotplus.graph_from_dot_data(out.getvalue())\n",
        "graph.write_png(\"tree.png\")\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iSXs1tay8Wq",
        "colab_type": "text"
      },
      "source": [
        "# 4. Kaggle evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mclUVPCAy8Ws",
        "colab_type": "text"
      },
      "source": [
        "Let's make a Kaggle submission:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS-kgBKAy8Wt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#code for submission\n",
        "predictions_test = cls_DT.predict_proba(test)\n",
        "\n",
        "out_tmp = pd.DataFrame()\n",
        "out_tmp[\"GeneId\"] = test_index_col\n",
        "out_tmp[\"eyeDetection\"] = predictions_test[:,1]\n",
        "out_tmp.to_csv(\"submission_DT.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl8OTjbZKvMp",
        "colab_type": "text"
      },
      "source": [
        "How does the model perform on the testset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9DhKikmy8W2",
        "colab_type": "text"
      },
      "source": [
        "# 5. Hyperparamters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ_O4Ezmy8W3",
        "colab_type": "text"
      },
      "source": [
        "For our first submission we set the hyperparameter `max_depth=3`. Other values might result in lower log-loss on the testset. \n",
        "\n",
        "Since we don't have the testset labels we can only check this on the public leaderboard, which we can/should not do!\n",
        "\n",
        "So, we need to create our own testset (**not seen during training!**) with known class labels.\n",
        "\n",
        "Scikit-learn offers many options to do this. One of them is the `train_test_split` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-U7t7hZy8W7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_X, val_X, train_y, val_y = train_test_split(train_norm,train_labels,\n",
        "                                                  test_size=.2, random_state=random_seed)\n",
        "\n",
        "#train fold\n",
        "print(train_X.shape)\n",
        "print(train_y.shape)\n",
        "#validation fold\n",
        "print(val_X.shape)\n",
        "print(val_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEv2x3Smy8XB",
        "colab_type": "text"
      },
      "source": [
        "Fit a decision tree model with `max_depth=14` default paramters on the `train_X` data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx29OyFly8XD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#solution"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH9q5De8y8XI",
        "colab_type": "text"
      },
      "source": [
        "What is the accuracy and log-loss on `train_X`? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW3HjC5Dy8XJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#solution"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqHXtasSy8XO",
        "colab_type": "text"
      },
      "source": [
        "What is the accuracy and log-loss on `val_X`?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm7GVlo-y8XQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#solution"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1p535f2y8XY",
        "colab_type": "text"
      },
      "source": [
        "What do you see?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBRBR7Q4y8Xd",
        "colab_type": "text"
      },
      "source": [
        "The following code evaluates different values for this hyperparameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6avhhiiy8Xe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for maxdepth in range(1,20,1):\n",
        "    cls = DecisionTreeClassifier(max_depth=maxdepth)\n",
        "    cls.fit(train_X,train_y)\n",
        "    predictions_train = cls.predict(train_X)\n",
        "    predictions_val = cls.predict(val_X)\n",
        "    predictions_train_prob = cls.predict_proba(train_X)[:,1]\n",
        "    predictions_val_prob = cls.predict_proba(val_X)[:,1]\n",
        "    print(\"%i (%f) %f (%f) %f\"%(maxdepth,\n",
        "                                accuracy_score(predictions_train, train_y),accuracy_score(predictions_val, val_y),\n",
        "                                log_loss(train_y,predictions_train_prob),log_loss(val_y,predictions_val_prob)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7lmtHBQvuon",
        "colab_type": "text"
      },
      "source": [
        "What do you see?\n",
        "\n",
        "So, we have split the data into a train- and validationset. We can of course split the data in many different ways (different random seeds) resulting in different train- and validationsets.\n",
        "\n",
        "Let's try 5 different random seeds:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOxT7dLan5Nr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for run in range(5):\n",
        "  train_X, val_X, train_y, val_y = train_test_split(train_norm,train_labels,\n",
        "                                                  test_size=.8, random_state=run)\n",
        "  min_m = 100\n",
        "  best = None\n",
        "  for maxdepth in range(1,20,1):\n",
        "      cls = DecisionTreeClassifier(max_depth=maxdepth)\n",
        "      cls.fit(train_X,train_y)\n",
        "      predictions_val_prob = cls.predict_proba(val_X)[:,1]\n",
        "      m = log_loss(val_y,predictions_val_prob)\n",
        "      if m < min_m:\n",
        "        min_m = m\n",
        "        best = maxdepth\n",
        "  print(\"%i %f\"%(best,min_m))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQgyvTfTwN8G",
        "colab_type": "text"
      },
      "source": [
        "What do you see?\n",
        "\n",
        "The solution is to run several train-validations splits and average the performance.\n",
        "\n",
        "One popular method is cross-validation that uses each datapoint once as a testpoint.\n",
        "\n",
        "It works as follows:\n",
        "<br/>\n",
        "<br/>\n",
        "<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\"/>\n",
        "<br/>\n",
        "<br/>\n",
        "\n",
        "It is easy to run this in Scikit-learn:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhFFLaJ_sG9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "for maxdepth in range(1,10,1):\n",
        "    cls = DecisionTreeClassifier(max_depth=maxdepth)\n",
        "    predictions = cross_val_predict(cls,train_norm,train_labels,\n",
        "                                    cv=10,\n",
        "                                    method=\"predict_proba\")\n",
        "    print(\"%i %f\"%(maxdepth,log_loss(train_labels,predictions[:,1])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87qY5lFuyEms",
        "colab_type": "text"
      },
      "source": [
        "We can do this in two lines of code with the `GridSearchCV` module:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgVOx2SLsIGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {\n",
        "    'max_depth':range(1,10)\n",
        "    }\n",
        "\n",
        "GSCV = GridSearchCV(cls_DT, params,\n",
        "                    cv=10,\n",
        "                    scoring=\"neg_log_loss\",\n",
        "                    verbose=1).fit(train_norm,train_labels)\n",
        "\n",
        "print(GSCV.best_estimator_)\n",
        "print(GSCV.best_score_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUgY_6Ply8X3",
        "colab_type": "text"
      },
      "source": [
        "Play with the hyperparameters in a Template notebook and make some Kaggle submissions.\n",
        "\n",
        "# 5. Ensemble learning: bagging\n",
        "\n",
        "We have seen that bias and variance play an important role in Machine Learning. \n",
        "\n",
        "Let's first see what bagging can do for our dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m16JfepBy8X4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "cls = BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=random_seed)\n",
        "                                                            \n",
        "cls.fit(train_X,train_y)\n",
        "predictions_train = cls.predict(train_X)\n",
        "predictions_val = cls.predict(val_X)\n",
        "print(\"Accuracy: (%f) %f\"%(accuracy_score(predictions_train, train_y),accuracy_score(predictions_val, val_y)))\n",
        "predictions_train_prob = cls.predict_proba(train_X)\n",
        "predictions_val_prob = cls.predict_proba(val_X)\n",
        "print(\"Log-loss: (%f) %f\"%(log_loss(train_y,predictions_train_prob[:,1]),log_loss(val_y,predictions_val_prob[:,1])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsA-43z8y8X9",
        "colab_type": "text"
      },
      "source": [
        "With the `RandomForestClassifier` the variance of the decision tree is reduced also by selecting features for decision tree contruction at random. Let's see how far we get with default hyperparameter values.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIfSnbtGy8X-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "cls = RandomForestClassifier(random_state=random_seed)\n",
        "\n",
        "cls.fit(train_X,train_y)\n",
        "predictions_train = cls.predict(train_X)\n",
        "predictions_val = cls.predict(val_X)\n",
        "print(\"Accuracy: (%f) %f\"%(accuracy_score(predictions_train, train_y),accuracy_score(predictions_val, val_y)))\n",
        "predictions_train_prob = cls.predict_proba(train_X)\n",
        "predictions_val_prob = cls.predict_proba(val_X)\n",
        "print(\"Log-loss: (%f) %f\"%(log_loss(train_y,predictions_train_prob[:,1]),log_loss(val_y,predictions_val_prob[:,1])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn-e3Gujy8Ya",
        "colab_type": "text"
      },
      "source": [
        "# 6. Ensemble learning: boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI1CdRroy8Yc",
        "colab_type": "text"
      },
      "source": [
        "How about the `GradientBoostingClassifier`?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hR1KZv_y8Yd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#solution"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIg3wxby4gOd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}