{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge pydotplus00\n",
    "#conda install -c gafortiby sklearn-pandas \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "random_seed = 123\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"header.png\" alt=\"drawing\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A multi-channel electroencephalography (EEG) system enables a broad range of applications including neurotherapy, biofeedback, and brain computer interfacing. It has 14 EEG channels with names based on the International 10-20 locations: AF3, F7, F3, FC5, T7, P7, O1, O2, P8, T8, FC6, F4, F8, AF4:\n",
    "\n",
    "<img src=\"EEG.png\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "The dataset you will analyse is created with the [Emotiv EPOC+](https://www.emotiv.com/product/emotiv-epoc-14-channel-mobile-eeg). All data is from one continuous EEG measurement with the Emotiv EEG Neuroheadset. The duration of the measurement was 117 seconds. The eye state was detected via a camera during the EEG measurement and later added manually to the file after analyzing the video frames. A '1' indicates the eye-closed and '0' the eye-open state.(*Source: Oliver Roesler, Stuttgart, Germany*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the Pandas library and reading the train set file into a Pandas DataFrame called `train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the test set file into a Pandas DataFrame called `test`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first 5 rows of the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see 14 columns that each correspond to the measurements of one EEG channel. Each row is a time-point. The column `eyeDetection` annotates each time-point with '1' when the eye was recorded as closed and '0' when the eye was recorded as open.  \n",
    "\n",
    "Our goal is to build a predictive model from the train set that is able to accuratly predict the eye states in the test set from the 14 channel measurements. \n",
    "\n",
    "Show the last 10 rows of the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a blind test so the `eyeDetection` column is not available in the test set. The test set does contain a column called `index` that you will need to send your predictions to the Kaggle website.\n",
    "\n",
    "Let's look at the shape of the train and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many time-points does the test set contain?\n",
    "\n",
    "We can get some general statistics about the train set using the DataFrame `.describe()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Building a decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn `DecisionTreeClassifier` class computes a decision tree predictive model from a data set. To get all the options for learning you can simply type: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "help(DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a decision tree model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_DT = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a decision tree model with default values for the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_DT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit the train set we need to have our features and labels in different DataFrames. I call them `X` and `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.sample(frac=1).copy()\n",
    "y = X.pop(\"eyeDetection\")\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit the train set in just one line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_DT.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well the decision tree performs on the train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "predictions = cls_DT.predict(X)\n",
    "print(\"Accuracy: %f\"%(accuracy_score(predictions, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! With just a few lines of code a get perfect accuracy on my data set!\n",
    "\n",
    "These are the predictions for the first 20 data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code plots the fitted decision tree `cls` as a `tree.png` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from io import StringIO\n",
    "from IPython.display import Image, display\n",
    "import pydotplus\n",
    "\n",
    "out = StringIO()\n",
    "tree.export_graphviz(cls_DT, out_file=out)\n",
    "graph=pydotplus.graph_from_dot_data(out.getvalue())\n",
    "graph.write_png(\"tree.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Kaggle evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a Kaggle submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for submission\n",
    "index_col = test.pop(\"index\")\n",
    "predictions_test = cls_DT.predict(test)\n",
    "out_tmp = pd.DataFrame()\n",
    "out_tmp[\"index\"] = index_col\n",
    "out_tmp[\"eyeDetection\"] = predictions_test\n",
    "out_tmp.to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. The test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't have the test set `eyeDetection` labels we cannot check overfitting on the test set before submitting our predictions. So we need to create our own test set (**not seen during training!**) with known class labels.\n",
    "\n",
    "Scikit-learn offers many options to do this. One of them is the `train_test_split` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X,y,test_size=.2, random_state=random_seed)\n",
    "\n",
    "#train fold\n",
    "print(train_X.shape)\n",
    "print(train_y.shape)\n",
    "#validation fold\n",
    "print(val_X.shape)\n",
    "print(val_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a decision tree model with default paramters on the `train_X` data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the accuracy on `train_X`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the accuracy on `val_X`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you see?\n",
    "\n",
    "\n",
    "# 4. Hyperparameters\n",
    "\n",
    "Let's look at the  `DecisionTreeClassifier` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are many parameters (**hyperparameters**) that influence the contruction of a decision tree. One of them is `max_depth` that limits the depth of the contructed decicison tree. The following code evaluates different values for this hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for maxdepth in range(3,24,1):\n",
    "    cls = DecisionTreeClassifier(max_depth=maxdepth)\n",
    "    cls.fit(train_X,train_y)\n",
    "    predictions_train = cls.predict(train_X)\n",
    "    predictions_val = cls.predict(val_X)\n",
    "    print(\"%i %f %f\"%(maxdepth,accuracy_score(predictions_train, train_y),accuracy_score(predictions_val, val_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a decision tree with `max_depth` equal to `None` and print the first 20 predictions on the train set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are binary class predictions (0 or 1). Sometimes it makes sense to predict the probalbility of a data point to belong to a certain class (typically class 1 which is also known as the positive class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = cls_DT.predict_proba(train_X)\n",
    "print(predictions_train[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frist column shows the probability of a data point to belong to class 0 and the second column shows the probability of a data point to belong to class 1. What do you see? Why is this?\n",
    "\n",
    "Can you contruct a decision tree that does not just output 0 or 1 probabilities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with the hyperparameters in a Template notebook and make some Kaggle submissions.\n",
    "\n",
    "# 5. Ensemble learning: bagging\n",
    "\n",
    "We have seen that bias and variance play an important role in Machine Learning. Let's first see what bagging can do for our data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "cls_bagged = BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=random_seed)\n",
    "                                                            \n",
    "cls_bagged.fit(train_X,train_y)\n",
    "predictions_train = cls_bagged.predict(train_X)\n",
    "predictions_val = cls_bagged.predict(val_X)\n",
    "print(\"%f %f\"%(accuracy_score(predictions_train, train_y),accuracy_score(predictions_val, val_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `RandomForestClassifier` the variance of the decision tree is reduced also by selecting features for decision tree contruction at random. Let's see how far we get with default hyperparameter values.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "cls_RF = RandomForestClassifier(random_state=random_seed)\n",
    "cls_RF.fit(train_X,train_y)\n",
    "predictions_train = cls_RF.predict(train_X)\n",
    "predictions_val = cls_RF.predict(val_X)\n",
    "print(\"%f %f\"%(accuracy_score(predictions_train, train_y),accuracy_score(predictions_val, val_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what we can do with the `RandomForestClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when you increase the `n_estimators` hyperparameter? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Ensemble learning: boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about the `GradientBoostingClassifier`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solution\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "cls_GB = GradientBoostingClassifier(random_state=random_seed,max_depth=10)\n",
    "cls_GB.fit(train_X,train_y)\n",
    "predictions_train = cls_GB.predict(train_X)\n",
    "predictions_val = cls_GB.predict(val_X)\n",
    "print(\"%f %f\"%(accuracy_score(predictions_train, train_y),accuracy_score(predictions_val, val_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Logistic regression\n",
    "\n",
    "Before we evaluate other Machine Learning algorithms such as logistic regression we need to look at the distributions and scales of the features. In fact, this should always be the first thing to do when analyzing a new data set.\n",
    "\n",
    "In Pandas we can create a boxplot for each of the features as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we see?\n",
    "\n",
    "There are many approaches to outlier detection and removal. Let's use a simple one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "print(X.shape)\n",
    "tmp = (np.abs(stats.zscore(X)) < 3).all(axis=1)\n",
    "X_clean = X[tmp]\n",
    "y_clean = y[tmp]\n",
    "print(X_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clean.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should look much better.\n",
    "\n",
    "However, feature values between features are still at very different scales. We need to normalize them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_clean)\n",
    "X_norm = pd.DataFrame(scaler.transform(X_clean),columns=X_clean.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each feature is at a similar scale.\n",
    "\n",
    "Let's start with `LogisticRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X_norm,y_clean,test_size=.2, random_state=random_seed)\n",
    "\n",
    "cls_LR=LogisticRegression()\n",
    "cls_LR.fit(train_X,train_y)\n",
    "predictions_train = cls_LR.predict(train_X)\n",
    "predictions_val = cls_LR.predict(val_X)\n",
    "print(\"%f %f\"%(accuracy_score(predictions_train, train_y),accuracy_score(predictions_val, val_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this perform well?\n",
    "\n",
    "What could be the reason?\n",
    "\n",
    "# 8. Non-linear transformations\n",
    "\n",
    "What does linear classification mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "dataset_tmp = pd.read_csv('svm_example1.csv')\n",
    "\n",
    "sns.lmplot(x=\"x1\", y=\"x2\", data=dataset_tmp, hue='y', markers=['o','+'], \n",
    "           fit_reg=False, height=5, scatter_kws={\"s\": 80})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "compomics_import = imp.load_source('compomics_import', 'compomics_import.py')\n",
    "\n",
    "X_tmp = dataset_tmp.copy()\n",
    "y_tmp = X_tmp.pop('y')\n",
    "\n",
    "cls_LR.fit(X_tmp,y_tmp)\n",
    "\n",
    "sns.lmplot(x=\"x1\", y=\"x2\", data=dataset_tmp, hue='y', markers=['o','+'], \n",
    "           fit_reg=False, height=5, scatter_kws={\"s\": 80})\n",
    "compomics_import.plot_svm_decision_function(cls_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "polynomial_features = PolynomialFeatures(degree=2)\n",
    "model = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"LR\", cls_LR)])\n",
    "model.fit(X_tmp,y_tmp)\n",
    "\n",
    "sns.lmplot(x=\"x1\", y=\"x2\", data=dataset_tmp, hue='y', markers=['o','+'], \n",
    "           fit_reg=False, height=5, scatter_kws={\"s\": 80})\n",
    "compomics_import.plot_svm_decision_function(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when you increase the degree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would this work on our dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle submission?\n",
    "\n",
    "The following code ranks the features based on the value of the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(cls_LR.coef_[0])[::-1]\n",
    "fnames = poly.get_feature_names()\n",
    "for i in range(len(fnames)):\n",
    "    print(\"%f\\t%s\"%(cls_LR.coef_[0][indices[i]],fnames[indices[i]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is wrong with this code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Support Vector Machines\n",
    "\n",
    "Let's see what an SVM can do with an RBF kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(kernel='poly',C=1,degree=2)\n",
    "model.fit(X_tmp,y_tmp)\n",
    "\n",
    "sns.lmplot(x=\"x1\", y=\"x2\", data=dataset_tmp, hue='y', markers=['o','+'], \n",
    "           fit_reg=False, height=5, scatter_kws={\"s\": 80})\n",
    "compomics_import.plot_svm_decision_function(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2_tmp = pd.read_csv(\"svm_example2.csv\")\n",
    "X_tmp = dataset2_tmp.copy()\n",
    "y_tmp = X_tmp.pop('y')\n",
    "\n",
    "model = SVC(kernel='rbf',C=1,gamma=1)\n",
    "model.fit(X_tmp,y_tmp)\n",
    "\n",
    "sns.lmplot(x=\"x1\", y=\"x2\", data=dataset2_tmp, hue='y', markers=['o','+'], \n",
    "           fit_reg=False, height=5, scatter_kws={\"s\": 80})\n",
    "compomics_import.plot_svm_decision_function(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, val_X, train_y, val_y = train_test_split(X_norm,y_clean,test_size=.2, random_state=random_seed)\n",
    "\n",
    "cls_SVM = SVC(kernel='rbf')\n",
    "cls_SVM.fit(train_X,train_y)\n",
    "predictions_train = cls_SVM.predict(train_X)\n",
    "predictions_val = cls_SVM.predict(val_X)\n",
    "print(\"%f %f\"%(accuracy_score(predictions_train, train_y),accuracy_score(predictions_val, val_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Neural Networks\n",
    "\n",
    "How about a Neural Network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "cls_MLP=MLPClassifier(hidden_layer_sizes=(60,10))\n",
    "cls_MLP.fit(train_X,train_y)\n",
    "predictions_train = cls_MLP.predict(train_X)\n",
    "predictions_val = cls_MLP.predict(val_X)\n",
    "print(\"%f %f\"%(accuracy_score(predictions_train, train_y),accuracy_score(predictions_val, val_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is feature normalization required?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, val_X, train_y, val_y = train_test_split(X,y,test_size=.2, random_state=random_seed)\n",
    "cls_MLP.fit(train_X,train_y)\n",
    "predictions_train = cls_MLP.predict(train_X)\n",
    "predictions_val = cls_MLP.predict(val_X)\n",
    "print(\"%f %f\"%(accuracy_score(predictions_train, train_y),accuracy_score(predictions_val, val_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
