{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sdgroeve/ML-course-VIB-2020/blob/master/Histone_marks_lr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWoU7h5NClGi"
   },
   "source": [
    "# Histone modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Krri7AXy8UM"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "random_seed = 123\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Suim-eikCrc8"
   },
   "source": [
    "[PyCaret](https://pycaret.org/) is an open source, low-code machine learning library in Python that allows you to go from preparing your data to deploying your model within minutes in your choice of notebook environment.\n",
    "\n",
    "It is not available by default, so we first install the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mv-SfPCv1Nzc"
   },
   "outputs": [],
   "source": [
    "! pip install pycaret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7DJU4Tny8Ua"
   },
   "source": [
    "# 1. The data\n",
    "\n",
    "Histone modifications play an important role in affecting gene regulation. Specific histone modifications at specific locations in or near the gene can alter the expression of genes. Predicting gene expression from histone modification signals is a widely studied research topic.\n",
    "\n",
    "In this competition you will predict gene expression levels (low=0, high=1) based on the presence of histone modifications at specific locations in the gene. You will try to find the model that learns the true underlying model best.\n",
    "\n",
    "For each gene a region of 10.000bp around the transcription start site of the gene is extracted (5000bp upstream and 5000bp downstream). This region is binned in 100 bins of 100bp. For each bin five core histone modification marks are counted [1].\n",
    "\n",
    "The dataset is compiled from the \"E047\" (Primary T CD8+ naive cells from peripheral blood) celltype from Roadmap Epigenomics Mapping Consortium (REMC) database.\n",
    "\n",
    "[1] Kundaje, A. et al. Integrative analysis of 111 reference human epige-\n",
    "nomes. Nature, 518, 317â€“330, 2015.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fEaghfDy8Ue"
   },
   "source": [
    "We start by loading the Pandas library and reading the datasets into Pandas DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alZpE70qy8Uh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"https://raw.githubusercontent.com/sdgroeve/ML-course-VIB-2020/master/data_train.csv\")\n",
    "test = pd.read_csv(\"https://raw.githubusercontent.com/sdgroeve/ML-course-VIB-2020/master/data_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WA6pU9qDy8U0"
   },
   "source": [
    "Let's look at a random sample of the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eBIQBcTTy8U1"
   },
   "outputs": [],
   "source": [
    "train.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ooHoxGTgCOA"
   },
   "source": [
    "An alternative visualization for this type of counting data is the heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1lAK99coCEOR"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(30,10))\n",
    "sns.heatmap(train.sample(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYHeBhf8y8VC"
   },
   "source": [
    "The label for each datapoint is in the `Label` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZuSUhLu4L8x"
   },
   "outputs": [],
   "source": [
    "train_labels = train.pop(\"Label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDJ7eAMch53J"
   },
   "source": [
    "Now `train` contains the feature columns only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DHXHPpo4SBG"
   },
   "source": [
    "Let's look at the number datapoints in each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ynUhI2de6TsU"
   },
   "outputs": [],
   "source": [
    "train_labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4eZlOyO6NNF"
   },
   "source": [
    "Let's look at `test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDV9hz5Ay8VD"
   },
   "outputs": [],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mao3WMV3y8VJ"
   },
   "source": [
    "This is a blind test so the `Label` column is not available in the test set. The test set does contain the `GeneId` column that is needed to send your predictions to the Kaggle website.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n4VjbA3ey8VN"
   },
   "outputs": [],
   "source": [
    "test_index_col = test.pop(\"GeneId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbiH1uzSy8VT"
   },
   "source": [
    "We can compute some decriptive statistics about the training set using the DataFrame `.describe()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sDAIJ23VkVWj"
   },
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHM6V5kfEGOU"
   },
   "source": [
    "We can plot these descriptive statistics to get a general overview of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iPg1lbiWy8VV"
   },
   "outputs": [],
   "source": [
    "for col in [\"count\",\"mean\",\"std\",\"min\",\"max\"]:\n",
    "  sns.distplot(pd.DataFrame(train.describe().transpose())[col])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oltcoOqH3TTD"
   },
   "source": [
    "We can use the Pandas `boxplot()` function to plot the feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjZd6rZs3ijb"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(22,8))\n",
    "train.boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsQjqutNtbP7"
   },
   "source": [
    "Let's plot these for each hisotone mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gv7vShayzrR8"
   },
   "outputs": [],
   "source": [
    "marks = {}\n",
    "for m in train.columns:\n",
    "  marks[m.split(\"_\")[0]] = True\n",
    "marks = list(marks.keys())\n",
    "marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XIwcZXHX0lO-"
   },
   "outputs": [],
   "source": [
    "for mark in marks:\n",
    "  cols = []\n",
    "  for m in train.columns:\n",
    "    if mark in m:\n",
    "      cols.append(m)\n",
    "  plt.figure(figsize=(22,8))    \n",
    "  train[cols].boxplot()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqPCOQL9nh8P"
   },
   "source": [
    "# 2. Our first model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBlctuXVE5w2"
   },
   "source": [
    "Let's fit a Logistic Regression model. We will first use the very popular (with good reason) [Scikit-learn](https://scikit-learn.org/stable/) library for that.\n",
    "\n",
    "First, we hold out 20% if the training data for independant validation.\n",
    "\n",
    "Next, we fit the modelparameters on the training set and evaluate the fitted model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "17CsBzlJlnVy"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(train,train_labels,\n",
    "                                                  test_size=.2, random_state=random_seed)\n",
    "\n",
    "cls = LogisticRegression()\n",
    "\n",
    "cls.fit(train_X,train_y)\n",
    "\n",
    "predictions_train = cls.predict(train_X)\n",
    "predictions_val = cls.predict(val_X)\n",
    "\n",
    "print(\"Accuracy: (%f) %f\"%(accuracy_score(predictions_train, train_y),accuracy_score(predictions_val, val_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "leHLQtnfobKd"
   },
   "source": [
    "# 3. How well does it perform in pratice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "slsToUzBojN3"
   },
   "outputs": [],
   "source": [
    "#code for submission\n",
    "predictions_test = cls.predict(test)\n",
    "\n",
    "to_write = pd.DataFrame()\n",
    "to_write[\"GeneId\"] = test_index_col\n",
    "to_write[\"Label\"] = predictions_test\n",
    "to_write.to_csv(\"submission1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jC5nNJuqqjZm"
   },
   "source": [
    "For the Kaggle competition your predictions are not evaluated by accuracy, but by log-loss:\n",
    "\n",
    "$$ - \\frac{1}{N} \\sum_{i=1}^N [y_{i} \\log \\, p_{i} + (1 - y_{i}) \\log \\, (1 - p_{i})],$$\n",
    "\n",
    "where $N$ is the number of datapoints, $y_i$ is the label of datapoint $i$, and $p_i$ is the prediction of the model expressed as a probability.\n",
    "\n",
    "Let's compute the log-loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4OHuU0oqtnC"
   },
   "outputs": [],
   "source": [
    "predictions_train = cls.predict(train_X)\n",
    "predictions_val = cls.predict(val_X)\n",
    "\n",
    "print(\"Log-loss: (%f) %f\"%(log_loss(train_y,predictions_train),log_loss(val_y,predictions_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kDGcmkU2KE5"
   },
   "source": [
    "# 4. Data pre-processing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNCXrS1x4Tf8"
   },
   "source": [
    "Let's scale all the features to [0,1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nGkTWm904gt9"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scaler_minmax = preprocessing.MinMaxScaler()\n",
    "scaler_minmax.fit(train)\n",
    "train_norm = pd.DataFrame(scaler_minmax.transform(train),columns=train.columns)\n",
    "train_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KYY3fLMq5RfO"
   },
   "outputs": [],
   "source": [
    "for mark in marks:\n",
    "  cols = []\n",
    "  for m in train_norm.columns:\n",
    "    if mark in m:\n",
    "      cols.append(m)\n",
    "  plt.figure(figsize=(22,8))    \n",
    "  train_norm[cols].boxplot()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnhndU6_HSNc"
   },
   "source": [
    "Now, did we improve the predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q10qEG_4prt4"
   },
   "outputs": [],
   "source": [
    "train_X, val_X, train_y, val_y = train_test_split(train_norm,train_labels,\n",
    "                                                  test_size=.2, random_state=random_seed)\n",
    "\n",
    "cls.fit(train_X,train_y)\n",
    "\n",
    "predictions_train = cls.predict(train_X)\n",
    "predictions_val = cls.predict(val_X)\n",
    "\n",
    "print(\"Accuracy: (%f) %f\"%(accuracy_score(predictions_train, train_y),accuracy_score(predictions_val, val_y)))\n",
    "\n",
    "predictions_train_prob = cls.predict_proba(train_X)\n",
    "predictions_val_prob = cls.predict_proba(val_X)\n",
    "\n",
    "print(\"Log-loss: (%f) %f\"%(log_loss(train_y,predictions_train_prob[:,1]),log_loss(val_y,predictions_val_prob[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8Se-pFmtM8h"
   },
   "source": [
    "# 5. Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_6D75_dH56P"
   },
   "source": [
    "From now on we will use the PyCaret library for fitting models. It provides many useful functions that make Machine Learning fun.\n",
    "\n",
    "To use PyCaret we first need to setup an environment that prepares the data for fitting and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PANvqYPqtJKL"
   },
   "outputs": [],
   "source": [
    "from pycaret.classification import *\n",
    "\n",
    "train_norm[\"Label\"] = train_labels\n",
    "setup(data=train_norm,target=\"Label\",numeric_features=list(train.columns)[:-1], train_size=0.8, silent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CuDdFLT6I4Fl"
   },
   "source": [
    "By default, PyCaret does not compute the log-loss metric. So, we add it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yU5WTl3YxDdv"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "add_metric('logloss', 'LogLoss', log_loss, greater_is_better=False, target=\"pred_proba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iddjK6eGJWK5"
   },
   "source": [
    "Next, we fit a model using `create_model` that is similar to `fit()` in Scikit-learn, except that it fits and evaluates (CV) on the 80% fitting part of the training set only. The output of this function is a score grid with CV scores by fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZBb1hnttNgi"
   },
   "outputs": [],
   "source": [
    "lr = create_model(\"lr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28zxsL3DKbc1"
   },
   "source": [
    "We just fitted a Logistic Regression model with a default value of 1.0 for the regularization parameter, as you can see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4r0Jur3jLAFQ"
   },
   "outputs": [],
   "source": [
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_i2VnnBOMRN"
   },
   "outputs": [],
   "source": [
    "result = predict_model(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsqyNod5NGCf"
   },
   "source": [
    "It is now easy to optimize (tune) this hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M2YCwnXxwBPB"
   },
   "outputs": [],
   "source": [
    "tuned_lr = tune_model(lr,tuner_verbose=2, n_iter=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fm3ivp1BNe6B"
   },
   "source": [
    "This is the 'optimal' value for the regularization hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vammb7DnNrRM"
   },
   "outputs": [],
   "source": [
    "tuned_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BxLhpxNw56f"
   },
   "source": [
    "We can now again test this model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-lj0b5WSmng"
   },
   "outputs": [],
   "source": [
    "result = predict_model(tuned_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mbVNvabxB2A"
   },
   "source": [
    "PyCaret offers some very useful tools for model validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_z7EAGbwGPp"
   },
   "outputs": [],
   "source": [
    "evaluate_model(tuned_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oh2FWcI2xRVZ"
   },
   "source": [
    "Finally, we can fit the optimal model onthe full training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-7I_MDU3kz1"
   },
   "outputs": [],
   "source": [
    "final_lr = finalize_model(tuned_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGk2jFvPxdBl"
   },
   "source": [
    "And the we can make predictions for the blind test set. PyCaret adds two columns to the test set: Label (the predicted class) and Score (the probability of class 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I7eLcj1k-U9m"
   },
   "outputs": [],
   "source": [
    "result = predict_model(final_lr,data=test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZPLJOB5O-20n"
   },
   "outputs": [],
   "source": [
    "to_write = pd.DataFrame()\n",
    "to_write[\"GeneId\"] = test_index_col\n",
    "to_write[\"Label\"] = result[\"Score\"]\n",
    "to_write.to_csv(\"submission2.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Histone_marks_lr.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
